{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: The Cross-Entropy Method\n",
    "\n",
    "A first encounter with an actual RL algorithm. As in, a better one (we hope) than the random agent we built in Chapter 2... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the net has linear output, even though we want to use the output as a set of probabilities. We _could_ add a softmax layer, but for improved numerical stability we'll instead use `CrossEntropyLoss()` in a minute, which requires raw numeric input and then does softmax & cross-entropy in the same step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple(\"Episode\", field_names=[\"reward\", \"steps\"])\n",
    "EpisodeStep = namedtuple(\"EpisodeStep\", field_names=[\"observation\", \"action\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1) # To get a \"probability\" from the raw output\n",
    "\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]) # Convert current observation to PyTorch tensor and pass as \"batch\" i.e. in a list\n",
    "        act_probs_v = sm(net(obs_v)) # Pass through NN and obtain probabilities\n",
    "        act_probs = act_probs_v.data.numpy()[0] # Convert bach to numpy array and take first batch element to get 1d array of probs\n",
    "\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) # Sample an action using those probabilities\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action) # Saving the observation *used to choose the action*\n",
    "        episode_steps.append(step)\n",
    "\n",
    "        if is_done:\n",
    "            # Add episode to store\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            # Prepare for next episode\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch # Quit the while loop and return the batch\n",
    "                batch = []\n",
    "        \n",
    "        # Prepare for next iteration\n",
    "        obs = next_obs\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards)) # Used for monitoring only\n",
    "\n",
    "    # Keep only observations and actions where reward was above specified percentile\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean # Final two just for tensorboard monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "#env = gym.wrappers.Monitor(env, directory=\"mon\", force=True) # record monitoring videos\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.679, reward_mean=18.6, rw_bound=21.5\n",
      "1: loss=0.681, reward_mean=24.6, rw_bound=27.5\n",
      "2: loss=0.676, reward_mean=24.2, rw_bound=24.0\n",
      "3: loss=0.671, reward_mean=29.9, rw_bound=32.5\n",
      "4: loss=0.658, reward_mean=28.9, rw_bound=37.0\n",
      "5: loss=0.666, reward_mean=28.4, rw_bound=33.5\n",
      "6: loss=0.649, reward_mean=25.1, rw_bound=29.0\n",
      "7: loss=0.652, reward_mean=32.2, rw_bound=36.5\n",
      "8: loss=0.637, reward_mean=34.4, rw_bound=44.5\n",
      "9: loss=0.639, reward_mean=37.9, rw_bound=44.0\n",
      "10: loss=0.605, reward_mean=47.5, rw_bound=55.0\n",
      "11: loss=0.616, reward_mean=34.6, rw_bound=38.0\n",
      "12: loss=0.604, reward_mean=44.5, rw_bound=49.0\n",
      "13: loss=0.582, reward_mean=38.9, rw_bound=48.0\n",
      "14: loss=0.598, reward_mean=49.1, rw_bound=54.0\n",
      "15: loss=0.603, reward_mean=45.9, rw_bound=48.5\n",
      "16: loss=0.597, reward_mean=49.1, rw_bound=52.0\n",
      "17: loss=0.561, reward_mean=51.9, rw_bound=57.0\n",
      "18: loss=0.570, reward_mean=48.2, rw_bound=56.0\n",
      "19: loss=0.564, reward_mean=58.1, rw_bound=59.5\n",
      "20: loss=0.535, reward_mean=59.3, rw_bound=63.5\n",
      "21: loss=0.557, reward_mean=57.4, rw_bound=65.0\n",
      "22: loss=0.554, reward_mean=67.9, rw_bound=77.5\n",
      "23: loss=0.546, reward_mean=69.6, rw_bound=75.5\n",
      "24: loss=0.541, reward_mean=61.8, rw_bound=67.0\n",
      "25: loss=0.528, reward_mean=62.4, rw_bound=74.5\n",
      "26: loss=0.520, reward_mean=73.8, rw_bound=74.0\n",
      "27: loss=0.521, reward_mean=73.9, rw_bound=83.5\n",
      "28: loss=0.552, reward_mean=80.3, rw_bound=92.5\n",
      "29: loss=0.517, reward_mean=84.7, rw_bound=102.5\n",
      "30: loss=0.503, reward_mean=76.4, rw_bound=87.5\n",
      "31: loss=0.506, reward_mean=71.7, rw_bound=83.5\n",
      "32: loss=0.524, reward_mean=82.1, rw_bound=91.0\n",
      "33: loss=0.521, reward_mean=69.2, rw_bound=77.0\n",
      "34: loss=0.515, reward_mean=85.5, rw_bound=102.0\n",
      "35: loss=0.512, reward_mean=88.9, rw_bound=116.0\n",
      "36: loss=0.491, reward_mean=77.9, rw_bound=86.0\n",
      "37: loss=0.502, reward_mean=100.1, rw_bound=115.5\n",
      "38: loss=0.507, reward_mean=71.8, rw_bound=77.0\n",
      "39: loss=0.494, reward_mean=84.1, rw_bound=92.5\n",
      "40: loss=0.513, reward_mean=80.6, rw_bound=88.0\n",
      "41: loss=0.502, reward_mean=94.4, rw_bound=105.5\n",
      "42: loss=0.502, reward_mean=100.5, rw_bound=106.0\n",
      "43: loss=0.489, reward_mean=131.4, rw_bound=139.5\n",
      "44: loss=0.509, reward_mean=133.3, rw_bound=161.0\n",
      "45: loss=0.502, reward_mean=141.9, rw_bound=159.0\n",
      "46: loss=0.494, reward_mean=129.4, rw_bound=152.5\n",
      "47: loss=0.504, reward_mean=157.2, rw_bound=173.5\n",
      "48: loss=0.494, reward_mean=160.4, rw_bound=189.0\n",
      "49: loss=0.498, reward_mean=159.8, rw_bound=177.5\n",
      "50: loss=0.489, reward_mean=163.9, rw_bound=178.0\n",
      "51: loss=0.494, reward_mean=165.2, rw_bound=192.0\n",
      "52: loss=0.471, reward_mean=164.7, rw_bound=197.0\n",
      "53: loss=0.475, reward_mean=176.6, rw_bound=200.0\n",
      "54: loss=0.482, reward_mean=160.7, rw_bound=184.5\n",
      "55: loss=0.482, reward_mean=159.3, rw_bound=199.0\n",
      "56: loss=0.481, reward_mean=178.5, rw_bound=200.0\n",
      "57: loss=0.477, reward_mean=178.1, rw_bound=200.0\n",
      "58: loss=0.483, reward_mean=165.3, rw_bound=189.0\n",
      "59: loss=0.485, reward_mean=180.9, rw_bound=200.0\n",
      "60: loss=0.470, reward_mean=185.2, rw_bound=200.0\n",
      "61: loss=0.475, reward_mean=194.6, rw_bound=200.0\n",
      "62: loss=0.474, reward_mean=191.4, rw_bound=200.0\n",
      "63: loss=0.480, reward_mean=180.7, rw_bound=200.0\n",
      "64: loss=0.462, reward_mean=197.5, rw_bound=200.0\n",
      "65: loss=0.468, reward_mean=198.3, rw_bound=200.0\n",
      "66: loss=0.469, reward_mean=200.0, rw_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "        iter_no, loss_v.item(), reward_m, reward_b\n",
    "    ))\n",
    "\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        writer.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrozenLake environment\n",
    "\n",
    "If we \"wrap\" our observations from a different environment (FrozenLake), we can apply exactly the same code as we just did with CartPole!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, shape, dtype=np.float32)\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low) # all 0.0\n",
    "        res[observation] = 1.0 # replace nth entry with 1.0\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two lines are altered below: the environment is wrapped, and the reward bound (i.e. when did we \"win\") is changed to an appropriate score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.381, reward_mean=0.0, rw_bound=0.0\n",
      "1: loss=1.383, reward_mean=0.1, rw_bound=0.0\n",
      "2: loss=1.380, reward_mean=0.0, rw_bound=0.0\n",
      "3: loss=1.362, reward_mean=0.0, rw_bound=0.0\n",
      "4: loss=1.372, reward_mean=0.1, rw_bound=0.0\n",
      "5: loss=1.373, reward_mean=0.0, rw_bound=0.0\n",
      "6: loss=1.374, reward_mean=0.0, rw_bound=0.0\n",
      "7: loss=1.365, reward_mean=0.1, rw_bound=0.0\n",
      "8: loss=1.346, reward_mean=0.0, rw_bound=0.0\n",
      "9: loss=1.334, reward_mean=0.0, rw_bound=0.0\n",
      "10: loss=1.365, reward_mean=0.1, rw_bound=0.0\n",
      "11: loss=1.319, reward_mean=0.0, rw_bound=0.0\n",
      "12: loss=1.338, reward_mean=0.0, rw_bound=0.0\n",
      "13: loss=1.357, reward_mean=0.0, rw_bound=0.0\n",
      "14: loss=1.279, reward_mean=0.0, rw_bound=0.0\n",
      "15: loss=1.282, reward_mean=0.0, rw_bound=0.0\n",
      "16: loss=1.311, reward_mean=0.0, rw_bound=0.0\n",
      "17: loss=1.314, reward_mean=0.0, rw_bound=0.0\n",
      "18: loss=1.354, reward_mean=0.0, rw_bound=0.0\n",
      "19: loss=1.303, reward_mean=0.0, rw_bound=0.0\n",
      "20: loss=1.262, reward_mean=0.1, rw_bound=0.0\n",
      "21: loss=1.255, reward_mean=0.0, rw_bound=0.0\n",
      "22: loss=1.244, reward_mean=0.0, rw_bound=0.0\n",
      "23: loss=1.252, reward_mean=0.1, rw_bound=0.0\n",
      "24: loss=1.262, reward_mean=0.0, rw_bound=0.0\n",
      "25: loss=1.258, reward_mean=0.0, rw_bound=0.0\n",
      "26: loss=1.276, reward_mean=0.0, rw_bound=0.0\n",
      "27: loss=1.365, reward_mean=0.1, rw_bound=0.0\n",
      "28: loss=1.294, reward_mean=0.1, rw_bound=0.0\n",
      "29: loss=1.232, reward_mean=0.1, rw_bound=0.0\n",
      "30: loss=1.292, reward_mean=0.0, rw_bound=0.0\n",
      "31: loss=1.263, reward_mean=0.0, rw_bound=0.0\n",
      "32: loss=1.342, reward_mean=0.0, rw_bound=0.0\n",
      "33: loss=1.255, reward_mean=0.1, rw_bound=0.0\n",
      "34: loss=1.265, reward_mean=0.0, rw_bound=0.0\n",
      "35: loss=1.250, reward_mean=0.0, rw_bound=0.0\n",
      "36: loss=1.242, reward_mean=0.0, rw_bound=0.0\n",
      "37: loss=1.310, reward_mean=0.0, rw_bound=0.0\n",
      "38: loss=1.279, reward_mean=0.1, rw_bound=0.0\n",
      "39: loss=1.343, reward_mean=0.0, rw_bound=0.0\n",
      "40: loss=1.311, reward_mean=0.0, rw_bound=0.0\n",
      "41: loss=1.300, reward_mean=0.0, rw_bound=0.0\n",
      "42: loss=1.280, reward_mean=0.0, rw_bound=0.0\n",
      "43: loss=1.300, reward_mean=0.1, rw_bound=0.0\n",
      "44: loss=1.355, reward_mean=0.0, rw_bound=0.0\n",
      "45: loss=1.244, reward_mean=0.0, rw_bound=0.0\n",
      "46: loss=1.218, reward_mean=0.0, rw_bound=0.0\n",
      "47: loss=1.348, reward_mean=0.0, rw_bound=0.0\n",
      "48: loss=1.308, reward_mean=0.0, rw_bound=0.0\n",
      "49: loss=1.234, reward_mean=0.0, rw_bound=0.0\n",
      "50: loss=1.256, reward_mean=0.0, rw_bound=0.0\n",
      "51: loss=1.353, reward_mean=0.0, rw_bound=0.0\n",
      "52: loss=1.267, reward_mean=0.1, rw_bound=0.0\n",
      "53: loss=1.270, reward_mean=0.0, rw_bound=0.0\n",
      "54: loss=1.245, reward_mean=0.0, rw_bound=0.0\n",
      "55: loss=1.178, reward_mean=0.0, rw_bound=0.0\n",
      "56: loss=1.265, reward_mean=0.0, rw_bound=0.0\n",
      "57: loss=1.186, reward_mean=0.1, rw_bound=0.0\n",
      "58: loss=1.334, reward_mean=0.0, rw_bound=0.0\n",
      "59: loss=1.264, reward_mean=0.0, rw_bound=0.0\n",
      "60: loss=1.293, reward_mean=0.1, rw_bound=0.0\n",
      "61: loss=1.113, reward_mean=0.0, rw_bound=0.0\n",
      "62: loss=1.171, reward_mean=0.1, rw_bound=0.0\n",
      "63: loss=1.223, reward_mean=0.1, rw_bound=0.0\n",
      "64: loss=1.186, reward_mean=0.0, rw_bound=0.0\n",
      "65: loss=1.167, reward_mean=0.0, rw_bound=0.0\n",
      "66: loss=1.082, reward_mean=0.0, rw_bound=0.0\n",
      "67: loss=1.105, reward_mean=0.0, rw_bound=0.0\n",
      "68: loss=1.144, reward_mean=0.1, rw_bound=0.0\n",
      "69: loss=1.148, reward_mean=0.1, rw_bound=0.0\n",
      "70: loss=1.032, reward_mean=0.0, rw_bound=0.0\n",
      "71: loss=1.108, reward_mean=0.1, rw_bound=0.0\n",
      "72: loss=1.244, reward_mean=0.0, rw_bound=0.0\n",
      "73: loss=1.215, reward_mean=0.0, rw_bound=0.0\n",
      "74: loss=1.067, reward_mean=0.0, rw_bound=0.0\n",
      "75: loss=1.115, reward_mean=0.1, rw_bound=0.0\n",
      "76: loss=1.279, reward_mean=0.0, rw_bound=0.0\n",
      "77: loss=1.082, reward_mean=0.0, rw_bound=0.0\n",
      "78: loss=1.219, reward_mean=0.0, rw_bound=0.0\n",
      "79: loss=1.087, reward_mean=0.0, rw_bound=0.0\n",
      "80: loss=1.125, reward_mean=0.0, rw_bound=0.0\n",
      "81: loss=1.102, reward_mean=0.1, rw_bound=0.0\n",
      "82: loss=1.123, reward_mean=0.0, rw_bound=0.0\n",
      "83: loss=1.156, reward_mean=0.0, rw_bound=0.0\n",
      "84: loss=1.144, reward_mean=0.0, rw_bound=0.0\n",
      "85: loss=1.169, reward_mean=0.0, rw_bound=0.0\n",
      "86: loss=1.270, reward_mean=0.0, rw_bound=0.0\n",
      "87: loss=1.033, reward_mean=0.0, rw_bound=0.0\n",
      "88: loss=1.092, reward_mean=0.0, rw_bound=0.0\n",
      "89: loss=1.064, reward_mean=0.0, rw_bound=0.0\n",
      "90: loss=1.106, reward_mean=0.0, rw_bound=0.0\n",
      "91: loss=1.088, reward_mean=0.1, rw_bound=0.0\n",
      "92: loss=1.145, reward_mean=0.1, rw_bound=0.0\n",
      "93: loss=1.050, reward_mean=0.0, rw_bound=0.0\n",
      "94: loss=1.101, reward_mean=0.0, rw_bound=0.0\n",
      "95: loss=1.219, reward_mean=0.0, rw_bound=0.0\n",
      "96: loss=1.093, reward_mean=0.0, rw_bound=0.0\n",
      "97: loss=1.085, reward_mean=0.0, rw_bound=0.0\n",
      "98: loss=1.042, reward_mean=0.1, rw_bound=0.0\n",
      "99: loss=1.042, reward_mean=0.1, rw_bound=0.0\n",
      "100: loss=1.066, reward_mean=0.0, rw_bound=0.0\n",
      "101: loss=1.154, reward_mean=0.0, rw_bound=0.0\n",
      "102: loss=1.121, reward_mean=0.0, rw_bound=0.0\n",
      "103: loss=1.066, reward_mean=0.0, rw_bound=0.0\n",
      "104: loss=1.016, reward_mean=0.0, rw_bound=0.0\n",
      "105: loss=1.086, reward_mean=0.0, rw_bound=0.0\n",
      "106: loss=1.055, reward_mean=0.1, rw_bound=0.0\n",
      "107: loss=1.070, reward_mean=0.0, rw_bound=0.0\n",
      "108: loss=1.129, reward_mean=0.0, rw_bound=0.0\n",
      "109: loss=1.084, reward_mean=0.0, rw_bound=0.0\n",
      "110: loss=1.121, reward_mean=0.0, rw_bound=0.0\n",
      "111: loss=1.015, reward_mean=0.0, rw_bound=0.0\n",
      "112: loss=0.998, reward_mean=0.0, rw_bound=0.0\n",
      "113: loss=0.993, reward_mean=0.0, rw_bound=0.0\n",
      "114: loss=1.026, reward_mean=0.0, rw_bound=0.0\n",
      "115: loss=1.033, reward_mean=0.0, rw_bound=0.0\n",
      "116: loss=0.968, reward_mean=0.1, rw_bound=0.0\n",
      "117: loss=0.948, reward_mean=0.0, rw_bound=0.0\n",
      "118: loss=1.065, reward_mean=0.0, rw_bound=0.0\n",
      "119: loss=1.099, reward_mean=0.1, rw_bound=0.0\n",
      "120: loss=1.109, reward_mean=0.0, rw_bound=0.0\n",
      "121: loss=0.926, reward_mean=0.0, rw_bound=0.0\n",
      "122: loss=1.002, reward_mean=0.0, rw_bound=0.0\n",
      "123: loss=0.989, reward_mean=0.0, rw_bound=0.0\n",
      "124: loss=0.956, reward_mean=0.0, rw_bound=0.0\n",
      "125: loss=1.055, reward_mean=0.1, rw_bound=0.0\n",
      "126: loss=0.949, reward_mean=0.0, rw_bound=0.0\n",
      "127: loss=1.136, reward_mean=0.0, rw_bound=0.0\n",
      "128: loss=0.990, reward_mean=0.0, rw_bound=0.0\n",
      "129: loss=1.001, reward_mean=0.0, rw_bound=0.0\n",
      "130: loss=0.938, reward_mean=0.1, rw_bound=0.0\n",
      "131: loss=0.987, reward_mean=0.0, rw_bound=0.0\n",
      "132: loss=0.919, reward_mean=0.1, rw_bound=0.0\n",
      "133: loss=1.034, reward_mean=0.0, rw_bound=0.0\n",
      "134: loss=1.112, reward_mean=0.1, rw_bound=0.0\n",
      "135: loss=1.054, reward_mean=0.1, rw_bound=0.0\n",
      "136: loss=0.983, reward_mean=0.0, rw_bound=0.0\n",
      "137: loss=0.979, reward_mean=0.0, rw_bound=0.0\n",
      "138: loss=1.118, reward_mean=0.0, rw_bound=0.0\n",
      "139: loss=1.150, reward_mean=0.0, rw_bound=0.0\n",
      "140: loss=1.056, reward_mean=0.1, rw_bound=0.0\n",
      "141: loss=1.027, reward_mean=0.0, rw_bound=0.0\n",
      "142: loss=0.968, reward_mean=0.1, rw_bound=0.0\n",
      "143: loss=1.154, reward_mean=0.0, rw_bound=0.0\n",
      "144: loss=0.914, reward_mean=0.0, rw_bound=0.0\n",
      "145: loss=1.082, reward_mean=0.0, rw_bound=0.0\n",
      "146: loss=1.036, reward_mean=0.0, rw_bound=0.0\n",
      "147: loss=1.036, reward_mean=0.0, rw_bound=0.0\n",
      "148: loss=1.193, reward_mean=0.1, rw_bound=0.0\n",
      "149: loss=1.121, reward_mean=0.0, rw_bound=0.0\n",
      "150: loss=1.146, reward_mean=0.0, rw_bound=0.0\n",
      "151: loss=1.118, reward_mean=0.1, rw_bound=0.0\n",
      "152: loss=1.034, reward_mean=0.0, rw_bound=0.0\n",
      "153: loss=1.057, reward_mean=0.0, rw_bound=0.0\n",
      "154: loss=1.053, reward_mean=0.0, rw_bound=0.0\n",
      "155: loss=1.114, reward_mean=0.0, rw_bound=0.0\n",
      "156: loss=1.160, reward_mean=0.0, rw_bound=0.0\n",
      "157: loss=1.191, reward_mean=0.1, rw_bound=0.0\n",
      "158: loss=1.037, reward_mean=0.1, rw_bound=0.0\n",
      "159: loss=1.097, reward_mean=0.0, rw_bound=0.0\n",
      "160: loss=1.189, reward_mean=0.1, rw_bound=0.0\n",
      "161: loss=1.195, reward_mean=0.0, rw_bound=0.0\n",
      "162: loss=1.178, reward_mean=0.0, rw_bound=0.0\n",
      "163: loss=1.253, reward_mean=0.0, rw_bound=0.0\n",
      "164: loss=1.087, reward_mean=0.0, rw_bound=0.0\n",
      "165: loss=1.282, reward_mean=0.1, rw_bound=0.0\n",
      "166: loss=1.120, reward_mean=0.0, rw_bound=0.0\n",
      "167: loss=1.186, reward_mean=0.1, rw_bound=0.0\n",
      "168: loss=1.168, reward_mean=0.0, rw_bound=0.0\n",
      "169: loss=1.132, reward_mean=0.0, rw_bound=0.0\n",
      "170: loss=1.188, reward_mean=0.0, rw_bound=0.0\n",
      "171: loss=1.189, reward_mean=0.0, rw_bound=0.0\n",
      "172: loss=1.237, reward_mean=0.0, rw_bound=0.0\n",
      "173: loss=1.202, reward_mean=0.1, rw_bound=0.0\n",
      "174: loss=1.233, reward_mean=0.0, rw_bound=0.0\n",
      "175: loss=1.187, reward_mean=0.0, rw_bound=0.0\n",
      "176: loss=1.275, reward_mean=0.1, rw_bound=0.0\n",
      "177: loss=1.242, reward_mean=0.1, rw_bound=0.0\n",
      "178: loss=1.188, reward_mean=0.0, rw_bound=0.0\n",
      "179: loss=1.166, reward_mean=0.0, rw_bound=0.0\n",
      "180: loss=1.114, reward_mean=0.1, rw_bound=0.0\n",
      "181: loss=1.186, reward_mean=0.0, rw_bound=0.0\n",
      "182: loss=1.147, reward_mean=0.0, rw_bound=0.0\n",
      "183: loss=1.286, reward_mean=0.0, rw_bound=0.0\n",
      "184: loss=1.179, reward_mean=0.0, rw_bound=0.0\n",
      "185: loss=1.181, reward_mean=0.0, rw_bound=0.0\n",
      "186: loss=1.105, reward_mean=0.0, rw_bound=0.0\n",
      "187: loss=1.165, reward_mean=0.0, rw_bound=0.0\n",
      "188: loss=1.249, reward_mean=0.0, rw_bound=0.0\n",
      "189: loss=1.205, reward_mean=0.1, rw_bound=0.0\n",
      "190: loss=1.277, reward_mean=0.0, rw_bound=0.0\n",
      "191: loss=1.268, reward_mean=0.0, rw_bound=0.0\n",
      "192: loss=1.232, reward_mean=0.0, rw_bound=0.0\n",
      "193: loss=1.195, reward_mean=0.0, rw_bound=0.0\n",
      "194: loss=1.222, reward_mean=0.1, rw_bound=0.0\n",
      "195: loss=1.219, reward_mean=0.0, rw_bound=0.0\n",
      "196: loss=1.227, reward_mean=0.0, rw_bound=0.0\n",
      "197: loss=1.217, reward_mean=0.0, rw_bound=0.0\n",
      "198: loss=1.227, reward_mean=0.0, rw_bound=0.0\n",
      "199: loss=1.189, reward_mean=0.1, rw_bound=0.0\n",
      "200: loss=1.199, reward_mean=0.0, rw_bound=0.0\n",
      "Failed\n"
     ]
    }
   ],
   "source": [
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\")) # wrapper applied to environment\n",
    "#env = gym.wrappers.Monitor(env, directory=\"mon\", force=True) # record monitoring videos\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-frozenlake-naive\")\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "        iter_no, loss_v.item(), reward_m, reward_b\n",
    "    ))\n",
    "\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "\n",
    "    if reward_m > 0.8: # this is the only other line changed\n",
    "        print(\"Solved!\")\n",
    "        writer.close()\n",
    "        break\n",
    "\n",
    "    if iter_no >= 200: # added to prevent running forever\n",
    "        print(\"Failed\")\n",
    "        writer.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't learn well though, because our reward system isn't set up in a good way for this problem!\n",
    "\n",
    "We currently only reward based on whether the agent reaches the goal (1.0) or not (0.0). But because it's super rare for that to happen with our initially-not-very-clever agent, we never end up with a set of \"good\" episodes to learn from and make improvements.\n",
    "\n",
    "So to use the cross-entropy method, we need:\n",
    "\n",
    "* Finite (and preverably short) training episodes\n",
    "* Total reward from episodes variable enough to differentiate between good and bad episodes\n",
    "* No requirement for intermediate monitoring of the agent, i.e. we don't know whether it succeeded or failed until the end\n",
    "\n",
    "And to improve things here, we could:\n",
    "\n",
    "* Use more episodes per batch (because successful episodes are rarer than with CartPole)\n",
    "* Apply a discount factor to the reward (to reward shorter episodes more highly)\n",
    "* Keep successful episodes for longer i.e. keep them in the \"filtered\" batch for multiple iterations (due to rare success again)\n",
    "* Decrease learning rate (to allow neural net to average more training samples)\n",
    "* Increase training time by a LOT (heh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
