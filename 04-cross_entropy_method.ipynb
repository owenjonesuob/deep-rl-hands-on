{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: The Cross-Entropy Method\n",
    "\n",
    "A first encounter with an actual RL algorithm. As in, a better one (we hope) than the random agent we built in Chapter 2... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the net has linear output, even though we want to use the output as a set of probabilities. We _could_ add a softmax layer, but for improved numerical stability we'll instead use `CrossEntropyLoss()` in a minute, which requires raw numeric input and then does softmax & cross-entropy in the same step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple(\"Episode\", field_names=[\"reward\", \"steps\"])\n",
    "EpisodeStep = namedtuple(\"EpisodeStep\", field_names=[\"observation\", \"action\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.SoftMax(dim=1) # To get a \"probability\" from the raw output\n",
    "\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]) # Convert current observation to PyTorch tensor and pass as \"batch\" i.e. in a list\n",
    "        act_probs_v = sm(net(obs_v)) # Pass through NN and obtain probabilities\n",
    "        act_probs = act_probs_v.data.numpy()[0] # Convert bach to numpy array and take first batch element to get 1d array of probs\n",
    "\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) # Sample an action using those probabilities\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action) # Saving the observation *used to choose the action*\n",
    "        episode_steps.append(step)\n",
    "\n",
    "        if is_done:\n",
    "            # Add episode to store\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            # Prepare for next episode\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch # Quit the while loop and return the batch\n",
    "                batch = []\n",
    "        \n",
    "        # Prepare for next iteration\n",
    "        obs = next_obs\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards)) # Used for monitoring only\n",
    "\n",
    "    # Keep only observations and actions where reward was above specified percentile\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean # Final two just for tensorboard monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
